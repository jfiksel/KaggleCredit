\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps� with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}

\title{Kaggle my Baggle}
\author{Alex Kellogg, Bill DeRose, Jacob Fiksel, Lingge Li}
%\date{}							% Activate to display a given date or no date


\begin{document}
\maketitle
\section{Introduction}
	Defaulting on loans played an integral role in the crumbling of the financial industry in the great recession of 2007-2008. The housing bubble was booming and banks wanted a share in the lucrative profits, so they engaged in misdirected and extremely risky lending practices (subprime mortgages, for example, which involve lending money to borrowers with lower credit ratings). As more and more banks engaged in these loans, the competition grew fierce (and the stakes were extremely high), which resulted in the institutions further relaxing their standards of credit worthiness. Banks were lending out way more than they could pay back. Finally, the subprime lending caught up with the banks, and all at once, a sea of people who were all classified as likely to default did indeed default. A few months later, the world faced the harshest recession since the 1930s. As a result of this default spiral, banks and regulators have started to pay special attention to estimating the likelihood of default (as well as its potential maximal costs). 
Enter Kaggle.com "Give Me Some Credit" competition. In this statistical modeling competition, we are provided with data from 150,000 people in 12 dimensions, including revolving utilization, monthly income, debt ratio, age, number of dependents, number of times late in paying loans, and number of real estate loans. With all this data, the goal is to train an algorithm to classify people as one of two types: serious delinquency in 2 years, or not – essentially, we are looking to asses the likelihood of default based on the factors mentioned above.

Data Cleanup and Scrutiny

The first, and arguably the most important, thing we did was examine and interpret the data. It did not take long to realize that there are a plethora of “NA” values, an issue we would certainly need to address in order to have better predictions (see Imputation Section). After a little more scrutiny in the form of creating box plots of each of the respective variables, we noticed a number of outliers. For example, someone was aged 0 and another was receiving several million dollars in monthly income (what industry is that in, please?). As a group, we decided to look into each of the variables and really think about the feasibility of the outlying values that we were observing. In doing so, we came to a couple of major realizations that helped boost our Kaggle score. 
The first key observation, and certainly the most significant of the three, was that 96\% of our data had a value of 0 in the field which we were to predict. Thus, each time we trained our data, the majority of the training was done on observations that would ultimately yield a value of zero (ie we could be 96\% correct by guessing 0’s for all values). We were noticing a large number of false negatives (predicted not to default, but did indeed default – the worst kind of error in this case), and decided that it would behoove us to train more of the data on people who did indeed default. Thus, we decided to weight the observations with a value of 1 in serious delinquency in 2 years more heavily than those with a 0. The way we did this was by creating a vector of probabilities, and assigning observations with a value of 1 a higher probability of being selected than those with a value of 0. In manipulating these weights, we were able to take subsets of our data (with resampling) with 40\% of this training data having defaulted. 
Next, we observed a dependency between debt ratio and monthly income (the former is a function of the latter). We had noticed, in both the box plots and a scatter plot of debt ratio vs monthly income, that low and NA values of monthly income were highly associated with large debt ratios. Our main concern was with the NA values – which produced a large percentage of the outliers, and so we proceeded to replace the debt ratios given to NA for every given monthly income value of NA. This got rid of a large number of the number of outliers.

\section{Clean Up}
	\subsection{Outliers}
	We begin our exploration of the data with summary statistics and plots. One of 
	the first anomolies we noticed was the presence of the values $96$ and $98$ 
	in the predictor variables. After doing some digging, we realized that 
	these quantitative values were used to code for qualitative values such as 
	``failed to answer or ``not applicable". We simply replaced these observations with
	\tt NA\rm~and let the imputation deal with the rest.
	
	Upon further inspection, we noted that some of the values for \tt RevolvingUtilizationOfUnsecuredLines
	\subsection{Imputation}
	Some individuals in the data set did not have values for certain values. Instead of ignoring blank values, and thus potentially ignoring values that could be key in classifying individuals, we decided to choose between two imputation methods: K-nearest neighbors imputation, and gradient boosting imputation. To evaluate the two methods, we focused on computation time and perceived accuracy of the methods. The K-Nearest Neighbor computes a distance matrix, and finds a speicifed number of closest observations to the individual you are imputing the data for. It then finds the mean of the variable you are imputing from those individuals. Unforunately, the algorithm in R was unable to compute a distance matrix for a data set of the size we are working with, which would force us to use much smaller subsets to impute the data, thus decreasing the accuracy of our imputations. Gradient boosting, the method that we used for our data, treats each missing value as a regression problem and uses boosting to create regression trees and predict the value as a weighted sum of the predictions. The weights are created to minimize a loss function, thus improving accuracy. While using regression to find missing values for our data set may not have been the best model, gradient boosting allowed us to use the training data to impute the missing values in the test data, and was significantly computationally less expensive than k-nearest neighbors imputation. 
	\subsection{Naive Bayes}
	A simple algorithm we used to contribute to our final classification was a naive bayes classifier. It uses a strong assumptation that each class contributes independently to the final classification, and thus is "naive." For each data point it computes the posterior probability of the individual being in a certain class by taking the product of the prior probabilites and the likelihood functions of the data. It classifies the individual into the group with the highest posterior probability. The advantages of this algorithm is that it is computationally fast, and does not require a large training set. 
	\subsection{Random Forest}
	\subsection{Boosting}
	\subsection{Genetic Algorithm}
\section{Conclusions}



\end{document}  
